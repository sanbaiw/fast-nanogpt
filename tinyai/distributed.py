# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/pt7-ddp.ipynb.

# %% auto 0
__all__ = ['ddp_enabled', 'to_tensor', 'get_tokens', 'FSDataSet', 'FixedStepCallback', 'DDPCB', 'DDPGradAccuTrainCB']

# %% ../nbs/pt7-ddp.ipynb 1
import random, math, torch, numpy as np, matplotlib.pyplot as plt
from .model import *
from .learner import *
from .hooks import *
from .init import *
from .speedup import *
from .hyperparam import *
import fastcore.all as fc
from functools import partial
import time
import os

# %% ../nbs/pt7-ddp.ipynb 2
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# %% ../nbs/pt7-ddp.ipynb 3
from torch.distributed import init_process_group, destroy_process_group
from torch.nn.parallel import DistributedDataParallel as DDP
import torch.distributed as dist

# %% ../nbs/pt7-ddp.ipynb 4
ddp_enabled = dist.is_available() and int(os.environ.get("RANK", -1)) != -1

# %% ../nbs/pt7-ddp.ipynb 9
def to_tensor(f):
    def _f(*args, **kwargs):
        return torch.tensor(f(*args, **kwargs), dtype=torch.long)
    return _f

@to_tensor
def get_tokens(input_file):
    with open(input_file) as f:
        text = f.read()
    tokens = enc.encode(text)
    return tokens

# %% ../nbs/pt7-ddp.ipynb 12
from torch.utils.data import IterableDataset

class FSDataSet(IterableDataset):
    """ A dataset that loads data from a directory of files.
    """
    def __init__(
        self, data_dir, pattern=None, token_fn=get_tokens, T=32, num_proc=1, rank=0
    ):
        self.T = T
        self.num_proc = num_proc
        self.rank = rank
        self.pattern = pattern
        self.token_fn = token_fn

        self.shards = self.get_shards(data_dir, pattern)
        self.current_shard = 0
        self.reset()

    def reset(self):
        self.current_shard = 0
        # each process starts at a different offset corresponding to its rank
        self.current_pos = self.T * self.rank
        self.tokens = self.token_fn(self.shards[self.current_shard])

    def get_shards(self, data_dir, pattern=None):
        shards = os.listdir(data_dir)
        if pattern is not None:
            shards = [os.path.join(data_dir, s) for s in shards if pattern in s]
        shards = sorted(shards)
        return shards

    def step(self):
        # advance position
        self.current_pos += self.T * self.num_proc
        # if next step will go over the end of the shard, move to the next shard
        if self.current_pos + self.T * self.num_proc + 1 > len(self.tokens):
            self.current_shard = (self.current_shard + 1) % len(self.shards)
            self.tokens = self.token_fn(self.shards[self.current_shard])
            self.current_pos = self.T * self.rank

    def __iter__(self):
        return self
    
    def __next__(self):
        buf = self.tokens[self.current_pos : self.current_pos + self.T + 1]
        x = buf[:-1]
        y = buf[1:]

        self.step()
        return x, y

# %% ../nbs/pt7-ddp.ipynb 15
class FixedStepCallback(Callback):
    def __init__(self, step_count=50):
        self.step_count = step_count

    def after_batch(self, learn):
        if hasattr(learn, "opt"):
            if learn.opt._step_count >= self.step_count:
                raise CancelFitException()

# %% ../nbs/pt7-ddp.ipynb 21
class DDPCB(Callback):
    def __init__(self, local_rank, compile=True):
        self.compile = compile
        self.local_rank = local_rank

    def before_fit(self, learn):
        if self.compile:
            learn.model = torch.compile(learn.model)

        learn.model = DDP(learn.model, device_ids=[self.local_rank])

# %% ../nbs/pt7-ddp.ipynb 23
class DDPGradAccuTrainCB(GradAccuTrainCB):

    def backward(self, learn):
        if ddp_enabled:
            learn.model.require_backward_grad_sync = (
                learn._micro_step_count % self.accu_steps == (self.accu_steps - 1)
            )
        super().backward(learn)
