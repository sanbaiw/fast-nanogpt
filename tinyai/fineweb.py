# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/pt8-fineweb.ipynb.

# %% auto 0
__all__ = ['gpt2_encoder', 'eot', 'to_tokens_np', 'load_np_tokens', 'DataDirNotEmptyError', 'TokenShardWriter']

# %% ../nbs/pt8-fineweb.ipynb 7
import tiktoken
gpt2_encoder = tiktoken.get_encoding("gpt2")
eot = gpt2_encoder._special_tokens["<|endoftext|>"]  # end of text token


# tokenize a single document and returns a numpy array of uint16 tokens
def to_tokens_np(doc):
    tokens = [eot]  # eot separates documents, so every sequence starts with eot
    tokens.extend(gpt2_encoder.encode_ordinary(doc["text"]))  # ignore special tokens
    tokens_np = np.array(tokens, dtype=np.uint16)
    return tokens_np

def load_np_tokens(input_file):
    tokens = np.load(input_file)
    tokens = tokens.astype(np.int32)
    return tokens

# %% ../nbs/pt8-fineweb.ipynb 9
import numpy as np
import os
from tqdm import tqdm


class DataDirNotEmptyError(Exception):
    pass


class TokenShardWriter:
    def __init__(self, shard_size=2**20, data_dir=".", overwrite=False):
        self.shard_size = shard_size
        self.shard_index = 0
        self.tokens_buffer = np.empty((shard_size), dtype=np.uint16)
        self.shard_token_count = 0  # number of tokens in the current shard
        self.pbar = None
        self.data_dir = data_dir
        self.overwrite = overwrite

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        self.clear_buffer()
        if self.pbar is not None:
            self.pbar.close()

    def write(self, tokens):
        if not self.overwrite and os.listdir(self.data_dir):
            raise DataDirNotEmptyError("Data directory is not empty")
        return self._write_shard(tokens)

    def _write_shard(self, tokens):
        # current shard has enough space for incoming tokens
        if len(tokens) + self.shard_token_count <= self.shard_size:
            self.tokens_buffer[
                self.shard_token_count : self.shard_token_count + len(tokens)
            ] = tokens
            self.shard_token_count += len(tokens)
            if self.pbar is None:
                self.pbar = tqdm(
                    total=self.shard_size,
                    unit="tokens",
                    desc=f"Shard {self.shard_index}",
                )
            self.pbar.update(len(tokens))
            return

        # current shard has not enough space
        split = (
            "val" if self.shard_index == 0 else "train"
        )  # first shard is val, the rest is train
        filename = os.path.join(self.data_dir, f"{split}_{self.shard_index:06d}.npy")
        remainder = self.shard_size - self.shard_token_count
        # fill the current shard
        self.tokens_buffer[
            self.shard_token_count : self.shard_token_count + remainder
        ] = tokens[:remainder]
        self.pbar.update(remainder)
        self.dump_tokens_np(filename, self.tokens_buffer)
        # start a new shard
        self.pbar.close()
        self.shard_index += 1
        # add the remaining tokens to the new shard
        self.tokens_buffer[: len(tokens) - remainder] = tokens[remainder:]
        self.shard_token_count = len(tokens) - remainder
        self.pbar = tqdm(
            total=self.shard_size,
            unit="tokens",
            desc=f"Shard {self.shard_index}",
        )
        self.pbar.update(self.shard_token_count)

    def clear_buffer(self):
        if self.shard_token_count > 0:
            split = "val" if self.shard_index == 0 else "train"
            filename = os.path.join(
                self.data_dir, f"{split}_{self.shard_index:06d}.npy"
            )
            self.dump_tokens_np(filename, self.tokens_buffer[: self.shard_token_count])
            self.shard_token_count = 0

    def dump_tokens_np(self, filename, tokens):
        np.save(filename, tokens)
