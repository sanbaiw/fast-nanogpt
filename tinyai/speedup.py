# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/pt5-speedup.ipynb.

# %% auto 0
__all__ = ['torch_dtype_float16', 'TimeCallback', 'MixedPrecisionTrainCB', 'CompileCB', 'FastCausalSelfAttention', 'get_model']

# %% ../nbs/pt5-speedup.ipynb 1
import random, math, torch, numpy as np, matplotlib.pyplot as plt
from .model import *
from .learner import *
from .hooks import *
from .init import *
import fastcore.all as fc
from functools import partial
import time

# %% ../nbs/pt5-speedup.ipynb 2
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# %% ../nbs/pt5-speedup.ipynb 7
import time

class TimeCallback(Callback):
    def before_batch(self, learn):
        self.t0 = time.time()

    def _log(self, d):
        pass

    def after_batch(self, learn):
        torch.cuda.synchronize()
        t1 = time.time()
        dt = (t1 - self.t0) * 1000
        x, _ = learn.batch
        tokens_per_sec = x.shape[0] * x.shape[1] / (t1 - self.t0)

        print(
            f"step {learn.iter}, loss: {learn.loss.item():.2f}, time: {dt:.2f}msi, tok/sec: {tokens_per_sec:.0f}"
        )

# %% ../nbs/pt5-speedup.ipynb 15
torch_dtype_float16 = (
    torch.bfloat16
    if torch.cuda.is_bf16_supported()
    else torch.float16
)

# %% ../nbs/pt5-speedup.ipynb 16
class MixedPrecisionTrainCB(TrainCB):

    def predict(self, learn):
        with torch.autocast(device_type=default_device, enabled=learn.training, dtype=torch_dtype_float16):
            learn.preds, learn.loss = learn.model(*learn.batch)


# %% ../nbs/pt5-speedup.ipynb 24
class CompileCB(Callback):
    def before_fit(self, learn):
        learn.model = torch.compile(learn.model)

# %% ../nbs/pt5-speedup.ipynb 26
class FastCausalSelfAttention(CausalSelfAttention):

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        # calculate query, key, values for all heads in batch and move head forward to be the batch dim
        # nh is "number of heads", hs is "head size", and C (number of channels) = nh * hs
        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer
        qkv = self.c_attn(x)
        q, k, v = qkv.split(self.n_embd, dim=2)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, T, C) -> (B, T, nh, hs) -> (B, nh, T, hs)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        # attention (materializes the large (T,T) matrix for all the queries and keys)
        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
        # att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
        # att = F.softmax(att, dim=-1)
        # y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)
        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)
        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side
        # output projection
        y = self.c_proj(y)
        return y

# %% ../nbs/pt5-speedup.ipynb 31
def get_model():
    return GPT(GPTConfig(vocab_size=50304), proj=ResidualLinear, attn=FastCausalSelfAttention)
