# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/pt6-hyperparam.ipynb.

# %% auto 0
__all__ = ['use_fused_adam', 'NormGradTrainCB', 'get_lr', 'CosineLR', 'ScheduleCB', 'RecordCB', 'get_learner_lr',
           'get_learner_grad_norm', 'get_optimizer', 'GradAccuTrainCB', 'GradAccuScheduleCB', 'GradAccuRecordCB',
           'GradAccuLogCallback']

# %% ../nbs/pt6-hyperparam.ipynb 1
import random, math, torch, numpy as np, matplotlib.pyplot as plt
from .model import *
from .learner import *
from .hooks import *
from .init import *
from .speedup import *
import fastcore.all as fc
from functools import partial
import time

# %% ../nbs/pt6-hyperparam.ipynb 2
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# %% ../nbs/pt6-hyperparam.ipynb 11
class NormGradTrainCB(MixedPrecisionTrainCB):

    def backward(self, learn):
        learn.loss.backward()
        learn.grad_nom = nn.utils.clip_grad_norm_(learn.model.parameters(), 1.0)


# %% ../nbs/pt6-hyperparam.ipynb 16
def get_lr(step, max_lr, min_lr, warmup_steps, max_steps):
    # linear warmup stage
    if step < warmup_steps:
        # there is no point of 0 lr even in the first step
        return max_lr * (step + 1) / warmup_steps

    # steps exceeding annealing stage are clamped to min_lr
    if step > max_steps:
        return min_lr

    # cosine decay
    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)
    decay_ratio = min(max(0.0, decay_ratio), 1.0)
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coefficent starts from 1 and goes to 0

    return min_lr + (max_lr - min_lr) * coeff

# %% ../nbs/pt6-hyperparam.ipynb 18
class CosineLR(torch.optim.lr_scheduler.LRScheduler):
    def __init__( self, optimizer, warmup_steps, max_steps, ratio=0.1, last_epoch=-1, verbose=False):
        self.max_lr = optimizer.param_groups[0]["lr"]
        self.min_lr = self.max_lr * ratio
        self.warmup_steps = warmup_steps
        self.max_steps = max_steps
        super().__init__(optimizer=optimizer, last_epoch=last_epoch, verbose=verbose)

    def get_lr(self):
        return [
            get_lr(
                self._step_count,
                self.max_lr,
                self.min_lr,
                self.warmup_steps,
                self.max_steps,
            )
            for base_lr in self.base_lrs
        ]

# %% ../nbs/pt6-hyperparam.ipynb 19
class ScheduleCB(Callback):

    def __init__(self, schedule_fn):
        self.schedule_fn = schedule_fn

    def before_fit(self, learn):
        self.sched = self.schedule_fn(learn.opt)

    def after_step(self, learn):
        # scheduler step after the optimizer step
        self.sched.step()

# %% ../nbs/pt6-hyperparam.ipynb 20
class RecordCB(Callback):

    def __init__(self, **d):
        # d is a dictionary of items to record and functions to calculate them
        self.d = d

    def before_fit(self, learn):
        self.recs = {k: [] for k in self.d.keys()}

    def after_batch(self, learn):
        if not learn.training:
            return
        for k, v in self.d.items():
            self.recs[k].append(v(self, learn))
    
    def plot(self):
        for k, v in self.recs.items():
            plt.plot(v, label=k)
            plt.legend()
            plt.show()

# %% ../nbs/pt6-hyperparam.ipynb 21
def get_learner_lr(cb, learn):
    return learn.opt.param_groups[0]['lr']

# %% ../nbs/pt6-hyperparam.ipynb 22
def get_learner_grad_norm(cb, learn):
    return learn.grad_nom.detach().cpu()

# %% ../nbs/pt6-hyperparam.ipynb 34
import inspect

use_fused_adam = (
    "cuda" == default_device and "fused" in inspect.signature(optim.AdamW).parameters
)

# %% ../nbs/pt6-hyperparam.ipynb 36
def get_optimizer(params, lr, weight_decay=0.1, fused=True):
    params = [p for p in params if p.requires_grad]
    decay_params = [p for p in params if p.dim() >= 2]
    nodecay_params = [p for p in params if p.dim() < 2]
    optim_groups = [
        {'params': decay_params, 'weight_decay': weight_decay},
        {'params': nodecay_params, 'weight_decay': 0.0}
    ]
    return optim.AdamW(optim_groups, lr=lr, betas=(0.9, 0.95), eps=1e-8, fused=fused) 

# %% ../nbs/pt6-hyperparam.ipynb 48
class GradAccuTrainCB(MixedPrecisionTrainCB):

    def __init__(self, accu_steps=1):
        self.accu_steps = accu_steps

    def before_fit(self, learn):
        learn._micro_step_count = 0
        learn.accu_steps = self.accu_steps

    def after_predict(self, learn):
        learn.loss = learn.loss / self.accu_steps
        learn._micro_step_count += 1

    def after_backward(self, learn):
        if learn._micro_step_count % learn.accu_steps == 0:
            learn.grad_nom = nn.utils.clip_grad_norm_(learn.model.parameters(), 1.0)

    def step(self, learn):
        if learn._micro_step_count % learn.accu_steps == 0:
            super().step(learn)

    def zero_grad(self, learn):
        if learn._micro_step_count % learn.accu_steps == 0:
            super().zero_grad(learn)

# %% ../nbs/pt6-hyperparam.ipynb 49
class GradAccuScheduleCB(Callback):

    def __init__(self, schedule_fn):
        self.schedule_fn = schedule_fn

    def before_fit(self, learn):
        self.sched = self.schedule_fn(learn.opt)

    def after_step(self, learn):
        if hasattr(learn, "accu_steps"):
            if learn._micro_step_count % learn.accu_steps == 0:
               self.sched.step()
        else:
            self.sched.step()

# %% ../nbs/pt6-hyperparam.ipynb 50
class GradAccuRecordCB(Callback):

    def __init__(self, **d):
        # d is a dictionary of items to record and functions to calculate them
        self.d = d

    def before_fit(self, learn):
        self.recs = {k: [] for k in self.d.keys()}

    def after_batch(self, learn):
        if not learn.training:
            return
        if hasattr(learn, "accu_steps"):
            if learn._micro_step_count % learn.accu_steps != 0:
                return
        for k, v in self.d.items():
            self.recs[k].append(v(self, learn))
    
    def plot(self):
        for k, v in self.recs.items():
            plt.plot(v, label=k)
            plt.legend()
            plt.show()

# %% ../nbs/pt6-hyperparam.ipynb 51
class GradAccuLogCallback(Callback):
    def before_batch(self, learn):
        if learn._micro_step_count % learn.accu_steps != 0:
            return
        self.t0 = time.time()
        self.loss_accu = 0

    def _log(self, d):
        print(
            f"step {d['step']}, loss: {d['loss']:.2f}, time: {d['time']:.2f}msi, tok/sec: {d['tok/sec']:.0f}"
        )

    def reset(self):
        self.loss_accu = 0

    def after_batch(self, learn):
        self.loss_accu += learn.loss.detach()
        if learn._micro_step_count % learn.accu_steps != 0:
            return
        torch.cuda.synchronize() # wait for the GPU to finish work
        t1 = time.time()
        dt = (t1 - self.t0) * 1000
        x, _ = learn.batch
        # tokens_per_sec = x.shape[0] * x.shape[1] / (t1 - self.t0) 
        tokens_per_sec = x.shape[0] * x.shape[1] / (t1 - self.t0) * learn.accu_steps

        d = {
            "step": learn.opt._step_count,
            "time": dt,
            "loss": self.loss_accu,
            "tok/sec": tokens_per_sec,
        }

        self._log(d)
        self.reset()
