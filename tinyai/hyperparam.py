# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/pt6-hyperparam.ipynb.

# %% auto 0
__all__ = ['admw', 'NormGradTrainCB', 'get_lr', 'CosineLR', 'ScheduleCB', 'RecordCB']

# %% ../nbs/pt6-hyperparam.ipynb 1
import random, math, torch, numpy as np, matplotlib.pyplot as plt
from .model import *
from .learner import *
from .hooks import *
from .init import *
from .speedup import *
import fastcore.all as fc
from functools import partial
import time

# %% ../nbs/pt6-hyperparam.ipynb 2
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# %% ../nbs/pt6-hyperparam.ipynb 6
admw = partial(optim.AdamW, betas=(0.9, 0.95), eps=1e-8)

# %% ../nbs/pt6-hyperparam.ipynb 8
class NormGradTrainCB(TrainCB):

    def backward(self, learn):
        learn.loss.backward()
        learn.grad_nom = nn.utils.clip_grad_norm_(learn.model.parameters(), 1.0)


# %% ../nbs/pt6-hyperparam.ipynb 13
def get_lr(step, max_lr, min_lr, warmup_steps, max_steps):
    # linear warmup stage
    if step < warmup_steps:
        # there is no point of 0 lr even in the first step
        return max_lr * (step + 1) / warmup_steps

    # steps exceeding annealing stage are clamped to min_lr
    if step > max_steps:
        return min_lr

    # cosine decay
    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)
    decay_ratio = min(max(0.0, decay_ratio), 1.0)
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coefficent starts from 1 and goes to 0

    return min_lr + (max_lr - min_lr) * coeff

# %% ../nbs/pt6-hyperparam.ipynb 15
class CosineLR(torch.optim.lr_scheduler.LRScheduler):
    def __init__( self, optimizer, warmup_steps, max_steps, ratio=0.1, last_epoch=-1, verbose=False):
        self.max_lr = optimizer.param_groups[0]["lr"]
        self.min_lr = self.max_lr * ratio
        self.warmup_steps = warmup_steps
        self.max_steps = max_steps
        super().__init__(optimizer=optimizer, last_epoch=last_epoch, verbose=verbose)

    def get_lr(self):
        return [
            get_lr(
                self._step_count,
                self.max_lr,
                self.min_lr,
                self.warmup_steps,
                self.max_steps,
            )
            for base_lr in self.base_lrs
        ]

# %% ../nbs/pt6-hyperparam.ipynb 16
class ScheduleCB(Callback):

    def __init__(self, schedule_fn):
        self.schedule_fn = schedule_fn
    
    def before_fit(self, learn):
        self.sched = self.schedule_fn(learn.opt)

    def after_batch(self, learn):
        self.sched.step()

# %% ../nbs/pt6-hyperparam.ipynb 17
class RecordCB(Callback):

    def __init__(self, **d):
        # d is a dictionary of items to record and functions to calculate them
        self.d = d

    def before_fit(self, learn):
        self.recs = {k: [] for k in self.d.keys()}

    def after_batch(self, learn):
        if not learn.training:
            return
        for k, v in self.d.items():
            self.recs[k].append(v(self, learn))
    
    def plot(self):
        for k, v in self.recs.items():
            plt.plot(v, label=k)
            plt.legend()
            plt.show()

# %% ../nbs/pt6-hyperparam.ipynb 18
def _lr(cb, learn):
    return learn.opt.param_groups[0]['lr']

# %% ../nbs/pt6-hyperparam.ipynb 19
def _grad_norm(cb, learn):
    return learn.grad_nom.detach().cpu()
