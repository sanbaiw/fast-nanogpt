{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from typing import Mapping, Any, Tuple, List, Union\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataSet:\n",
    "    def __init__(self, x, y): self.x, self.y = x, y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "denc = tiktoken.get_encoding(\"gpt2\")\n",
    "input_file = f\"{cwd}/fast-nanogpt/input.txt\"\n",
    "with open(input_file) as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252,    11,\n",
       "          3285,   502,  2740,    13,   198,   198,  3237,    25,   198,  5248,\n",
       "           461,    11,  2740,    13,   198,   198,  5962, 22307,    25,   198,\n",
       "          1639,   389], device='cuda:0'),\n",
       " tensor([22307,    25,   198,  8421,   356,  5120,   597,  2252,    11,  3285,\n",
       "           502,  2740,    13,   198,   198,  3237,    25,   198,  5248,   461,\n",
       "            11,  2740,    13,   198,   198,  5962, 22307,    25,   198,  1639,\n",
       "           389,   477], device='cuda:0'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = denc.encode(text[:1000])\n",
    "B, T = 4, 32\n",
    "buf = torch.tensor(tokens[: B * T + 1]).to('cuda')\n",
    "x = buf[:-1].view(B, T)\n",
    "y = buf[1:].view(B, T)\n",
    "\n",
    "x[0], y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "x.shape, y.shape torch.Size([4, 32]) torch.Size([4, 32])\n"
     ]
    }
   ],
   "source": [
    "ds = DataSet(x, y)\n",
    "dl = DataLoader(ds, batch_size=4)\n",
    "iterdl = iter(dl)\n",
    "\n",
    "for i, (x, y) in enumerate(iterdl):\n",
    "    print(\"batch\", i)\n",
    "    print(\"x.shape, y.shape\", x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataLoaders:\n",
    "    def __init__(self, *dls):\n",
    "        self.train, self.valid = dls[:2]\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dd(cls, datasets, batch_size, **kwargs):\n",
    "        return cls(*[DataLoader(ds, batch_size=batch_size, **kwargs) for ds in datasets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32]) torch.Size([4, 32])\n"
     ]
    }
   ],
   "source": [
    "dls = DataLoaders.from_dd([ds, None], batch_size=4)\n",
    "for x, y in dls.train:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinyai.model import get_model\n",
    "model = get_model().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "overfit one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 10.982128143310547\n",
      "step 1 loss 7.473860740661621\n",
      "step 2 loss 6.256784439086914\n",
      "step 3 loss 4.943249225616455\n",
      "step 4 loss 4.460738658905029\n",
      "step 5 loss 3.7815935611724854\n",
      "step 6 loss 3.3974170684814453\n",
      "step 7 loss 2.9256558418273926\n",
      "step 8 loss 2.6330950260162354\n",
      "step 9 loss 2.3998520374298096\n",
      "step 10 loss 2.202265739440918\n",
      "step 11 loss 2.047623634338379\n",
      "step 12 loss 1.9105652570724487\n",
      "step 13 loss 1.8181020021438599\n",
      "step 14 loss 1.7237106561660767\n",
      "step 15 loss 1.636688232421875\n",
      "step 16 loss 1.5855993032455444\n",
      "step 17 loss 1.5355573892593384\n",
      "step 18 loss 1.4820191860198975\n",
      "step 19 loss 1.4466246366500854\n",
      "step 20 loss 1.4228850603103638\n",
      "step 21 loss 1.3980950117111206\n",
      "step 22 loss 1.3777544498443604\n",
      "step 23 loss 1.3592846393585205\n",
      "step 24 loss 1.3441938161849976\n",
      "step 25 loss 1.3351850509643555\n",
      "step 26 loss 1.325745940208435\n",
      "step 27 loss 1.316022276878357\n",
      "step 28 loss 1.30783212184906\n",
      "step 29 loss 1.3020973205566406\n",
      "step 30 loss 1.2977418899536133\n",
      "step 31 loss 1.2909226417541504\n",
      "step 32 loss 1.2861162424087524\n",
      "step 33 loss 1.2843577861785889\n",
      "step 34 loss 1.2802269458770752\n",
      "step 35 loss 1.2764517068862915\n",
      "step 36 loss 1.2740403413772583\n",
      "step 37 loss 1.2702240943908691\n",
      "step 38 loss 1.2682996988296509\n",
      "step 39 loss 1.266623616218567\n",
      "step 40 loss 1.2636067867279053\n",
      "step 41 loss 1.261276364326477\n",
      "step 42 loss 1.2592655420303345\n",
      "step 43 loss 1.2573940753936768\n",
      "step 44 loss 1.2551029920578003\n",
      "step 45 loss 1.2531388998031616\n",
      "step 46 loss 1.2508540153503418\n",
      "step 47 loss 1.248905897140503\n",
      "step 48 loss 1.2470568418502808\n",
      "step 49 loss 1.2441495656967163\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "for i in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    # forward the model\n",
    "    logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # if i % 10 == 0:\n",
    "    print(f\"step {i} loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from operator import attrgetter\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CancelFitException(Exception): pass\n",
    "class CancelBatchException(Exception): pass\n",
    "class CancelEpochException(Exception): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Callback:\n",
    "    order = 0\n",
    "\n",
    "\n",
    "def run_cbs(cbs, method_nm, learn=None):\n",
    "    for cb in sorted(cbs, key=attrgetter(\"order\")):\n",
    "        method = getattr(cb, method_nm, None)\n",
    "        if method is not None:\n",
    "            method(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class with_cbs:\n",
    "    def __init__(self, nm):\n",
    "        self.nm = nm\n",
    "\n",
    "    def __call__(self, f):\n",
    "        def _f(o, *args, **kwargs):\n",
    "            try:\n",
    "                o.callback(f\"before_{self.nm}\")\n",
    "                f(o, *args, **kwargs)\n",
    "                o.callback(f\"after_{self.nm}\")\n",
    "            except globals()[f\"Cancel{self.nm.title()}Exception\"]:\n",
    "                pass\n",
    "            finally:\n",
    "                o.callback(f\"cleanup_{self.nm}\")\n",
    "\n",
    "        return _f\n",
    "\n",
    "\n",
    "class Learner:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        dls=(0,),\n",
    "        lr=0.1,\n",
    "        cbs=None,\n",
    "        opt_func=optim.SGD,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.dls = dls\n",
    "        self.lr = lr\n",
    "        self.cbs = cbs if cbs else []\n",
    "        self.opt_func = opt_func\n",
    "\n",
    "    @with_cbs(\"batch\")\n",
    "    def _one_batch(self):\n",
    "        self.predict()\n",
    "        self.callback(\"after_predict\")\n",
    "        # self.get_loss()\n",
    "        # self.callback(\"after_loss\")\n",
    "        if self.training:\n",
    "            self.backward()\n",
    "            self.callback(\"after_backward\")\n",
    "            self.step()\n",
    "            self.callback(\"after_step\")\n",
    "            self.zero_grad()\n",
    "\n",
    "    @with_cbs(\"epoch\")\n",
    "    def _one_epoch(self):\n",
    "        for self.iter, self.batch in enumerate(self.dl):\n",
    "            self._one_batch()\n",
    "\n",
    "    def one_epoch(self, training=True):\n",
    "        self.model.train(training)\n",
    "        self.dl = self.dls.train if training else self.dls.valid\n",
    "        self._one_epoch()\n",
    "\n",
    "    @with_cbs(\"fit\")\n",
    "    def _fit(self, train, valid):\n",
    "        for self.epoch in self.epochs:\n",
    "            if train:\n",
    "                self.one_epoch(training=True)\n",
    "            if valid:\n",
    "                with torch.no_grad():\n",
    "                    self.one_epoch(False)\n",
    "\n",
    "    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):\n",
    "        if cbs is None:\n",
    "            cbs = []\n",
    "        for cb in cbs:\n",
    "            self.cbs.append(cb)\n",
    "        try:\n",
    "            self.n_epochs = n_epochs\n",
    "            self.epochs = range(n_epochs)\n",
    "            if lr is None:\n",
    "                lr = self.lr\n",
    "            if self.opt_func:\n",
    "                self.opt = self.opt_func(self.model.parameters(), lr)\n",
    "            self._fit(train, valid)\n",
    "        finally:\n",
    "            for cb in cbs:\n",
    "                self.cbs.remove(cb)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if name in (\"predict\", \"get_loss\", \"backward\", \"step\", \"zero_grad\"):\n",
    "            return partial(self.callback, name)\n",
    "        raise AttributeError(name)\n",
    "\n",
    "    def callback(self, method_nm):\n",
    "        run_cbs(self.cbs, method_nm, self)\n",
    "\n",
    "    @property\n",
    "    def training(self):\n",
    "        return self.model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "default_device = (\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "\n",
    "def to_device(x, device=default_device):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.to(device)\n",
    "    if isinstance(x, Mapping):\n",
    "        return {k: v.to(device) for k, v in x.items()}\n",
    "    return type(x)(to_device(o, device) for o in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DeviceCB(Callback):\n",
    "    \"\"\"Put model to device at the beginning of training, and put batch to device before each forward pass.\"\"\"\n",
    "\n",
    "    def __init__(self, device=default_device):\n",
    "        self.device = device\n",
    "\n",
    "    def before_fit(self, learn):\n",
    "        if hasattr(learn.model, \"to\"):\n",
    "            learn.model.to(self.device)\n",
    "\n",
    "    def before_batch(self, learn):\n",
    "        learn.batch = to_device(learn.batch, device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TrainCB(Callback):\n",
    "\n",
    "    def predict(self, learn):\n",
    "        # import pdb; pdb.set_trace()\n",
    "        learn.preds, learn.loss = learn.model(*learn.batch)\n",
    "        print(\"epoch\", learn.epoch, \"step\", learn.iter, \"loss\", learn.loss.item())\n",
    "\n",
    "    def backward(self, learn):\n",
    "        learn.loss.backward()\n",
    "\n",
    "    def step(self, learn):\n",
    "        learn.opt.step()\n",
    "\n",
    "    def zero_grad(self, learn):\n",
    "        learn.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [TrainCB(), DeviceCB()]\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 step 0 loss 10.979068756103516\n"
     ]
    }
   ],
   "source": [
    "lrn = Learner(model, dls=dls, opt_func=optim.AdamW, cbs=cbs, lr=3e-4)\n",
    "lrn.fit(1, valid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverfitLearner(Learner):\n",
    "\n",
    "    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None, n_repeat=50):\n",
    "        self.n_repeat = n_repeat\n",
    "        super().fit(n_epochs, train, valid, cbs, lr)\n",
    "\n",
    "    @with_cbs(\"epoch\")\n",
    "    def _one_epoch(self):\n",
    "        for self.iter in range(self.n_repeat):\n",
    "            for self.batch in self.dl:\n",
    "                self._one_batch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfit one batch with learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 step 0 loss 10.887825012207031\n",
      "epoch 0 step 1 loss 7.312341213226318\n",
      "epoch 0 step 2 loss 5.874819278717041\n",
      "epoch 0 step 3 loss 5.05126428604126\n",
      "epoch 0 step 4 loss 4.239614486694336\n",
      "epoch 0 step 5 loss 3.500850200653076\n",
      "epoch 0 step 6 loss 3.154188871383667\n",
      "epoch 0 step 7 loss 2.7710211277008057\n",
      "epoch 0 step 8 loss 2.4927163124084473\n",
      "epoch 0 step 9 loss 2.295257329940796\n",
      "epoch 0 step 10 loss 2.130744457244873\n",
      "epoch 0 step 11 loss 1.985034465789795\n",
      "epoch 0 step 12 loss 1.8574798107147217\n",
      "epoch 0 step 13 loss 1.7607990503311157\n",
      "epoch 0 step 14 loss 1.687723994255066\n",
      "epoch 0 step 15 loss 1.6228063106536865\n",
      "epoch 0 step 16 loss 1.5619274377822876\n",
      "epoch 0 step 17 loss 1.5092854499816895\n",
      "epoch 0 step 18 loss 1.471203327178955\n",
      "epoch 0 step 19 loss 1.4410830736160278\n",
      "epoch 0 step 20 loss 1.4119800329208374\n",
      "epoch 0 step 21 loss 1.38829505443573\n",
      "epoch 0 step 22 loss 1.3694020509719849\n",
      "epoch 0 step 23 loss 1.3536128997802734\n",
      "epoch 0 step 24 loss 1.3396373987197876\n",
      "epoch 0 step 25 loss 1.326297640800476\n",
      "epoch 0 step 26 loss 1.3175262212753296\n",
      "epoch 0 step 27 loss 1.309662103652954\n",
      "epoch 0 step 28 loss 1.3021068572998047\n",
      "epoch 0 step 29 loss 1.2953064441680908\n",
      "epoch 0 step 30 loss 1.2890526056289673\n",
      "epoch 0 step 31 loss 1.2852495908737183\n",
      "epoch 0 step 32 loss 1.2812283039093018\n",
      "epoch 0 step 33 loss 1.2763773202896118\n",
      "epoch 0 step 34 loss 1.2727338075637817\n",
      "epoch 0 step 35 loss 1.2694450616836548\n",
      "epoch 0 step 36 loss 1.2665016651153564\n",
      "epoch 0 step 37 loss 1.2634140253067017\n",
      "epoch 0 step 38 loss 1.2606239318847656\n",
      "epoch 0 step 39 loss 1.2577975988388062\n",
      "epoch 0 step 40 loss 1.2552490234375\n",
      "epoch 0 step 41 loss 1.252504825592041\n",
      "epoch 0 step 42 loss 1.2497857809066772\n",
      "epoch 0 step 43 loss 1.247087836265564\n",
      "epoch 0 step 44 loss 1.2446506023406982\n",
      "epoch 0 step 45 loss 1.2417685985565186\n",
      "epoch 0 step 46 loss 1.2387025356292725\n",
      "epoch 0 step 47 loss 1.2359164953231812\n",
      "epoch 0 step 48 loss 1.2327880859375\n",
      "epoch 0 step 49 loss 1.2290902137756348\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "lrn = OverfitLearner(model, dls=dls, opt_func=optim.AdamW, cbs=cbs, lr=3e-4)\n",
    "lrn.fit(1, valid=False, n_repeat=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
